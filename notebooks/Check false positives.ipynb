{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configurations from (in order) ['../config/default.ini', '../config/stacked.ini', '../config/titania_stacked.ini']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import params\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_folder = '../models/'\n",
    "\n",
    "model_filename = '1472001110_stacked/1472001110_stacked_epoch224.npz'\n",
    "model_path = os.path.join(model_folder, model_filename)\n",
    "\n",
    "params.params = params.Params(['../config/default.ini'] + \n",
    "                              ['../config/stacked.ini', '../config/titania_stacked.ini'])\n",
    "from params import params as P\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import resnet\n",
    "\n",
    "import patch_sampling\n",
    "from cparallel import ContinuousParallelBatchIterator\n",
    "\n",
    "input_var = T.tensor4('inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train samplers\n",
      "Loading validation samplers\n",
      "Loading samplers took 40.0372669697 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator, train_sampler, validation_sampler = (\n",
    "    \n",
    "    patch_sampling.prepare_custom_sampler(mini_subset=False, override_cache_size=1, return_samplers=True)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEFINE AND LOAD NETWORK\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "net = resnet.ResNet_FullPre_Wide(input_var, 4, 2)\n",
    "all_layers = lasagne.layers.get_all_layers(net)\n",
    "net = all_layers[-3]\n",
    "net = resnet.ResNet_Stacked_Old(net)\n",
    "\n",
    "with np.load(model_path) as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "\n",
    "lasagne.layers.set_all_param_values(net, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_fn = resnet.define_predict(net, input_var, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "X = [batch_size]*1000\n",
    "\n",
    "batch_gen = ContinuousParallelBatchIterator(validation_generator, \n",
    "                                            ordered=False, batch_size=1, multiprocess=False, n_producers=3)\n",
    "batch_gen.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "all_targets = []\n",
    "all_filenames = []\n",
    "\n",
    "all_loss = []\n",
    "all_pred = []\n",
    "all_binary_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(batch_gen(30))):\n",
    "    inputs, targets, filenames = batch\n",
    "    \n",
    "    loss, acc, pred_binary, pred = predict_fn(inputs, targets)\n",
    "    \n",
    "    for x in range(len(inputs)):\n",
    "        \n",
    "        filename = filenames[x]\n",
    "        target = targets[x]\n",
    "        p = pred[x]\n",
    "        p_binary = pred_binary[x]\n",
    "        l = loss[x]\n",
    "        \n",
    "        #all_inputs.append(inputs[x].astype(np.float16))\n",
    "        all_targets.append(target)\n",
    "        all_filenames.append(filename)\n",
    "        all_pred.append(p)\n",
    "        all_binary_pred.append(p_binary)\n",
    "        all_loss.append(l)\n",
    "            \n",
    "    del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(np.unique(filenames))\n",
    "\n",
    "file_loss = {}\n",
    "file_count = {}\n",
    "\n",
    "for filename, target, prediction, prediction_binary, loss in zip(all_filenames, all_targets, all_pred, all_binary_pred, all_loss):\n",
    "    \n",
    "    if filename not in file_loss:\n",
    "        file_loss[filename] = 0\n",
    "        file_count[filename] = 0\n",
    "        \n",
    "    file_loss[filename] += loss\n",
    "    file_count[filename] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average loss per file\n",
    "file_avg_loss = {f:file_loss[f]/file_count[f] for f in np.unique(filenames)}\n",
    "import operator\n",
    "file_loss_tups = file_avg_loss.iteritems()\n",
    "# Sort by loss\n",
    "file_loss_tups = list(reversed(sorted(file_loss_tups, key=operator.itemgetter(1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "misclassified = np.array(all_binary_pred) != np.array(all_targets)\n",
    "print \"Amount of wrong labels\", np.sum(misclassified), \"out of\", len(all_targets)\n",
    "\n",
    "#wrong_images = util.unzero_center(wrong_images, P.MEAN_PIXEL)\n",
    "#wrong_images = wrong_images.transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler_of_filename = {}\n",
    "\n",
    "all_samplers = []\n",
    "for x in validation_sampler.per_label_sampler_list.values():\n",
    "    all_samplers += x\n",
    "\n",
    "for sampler in all_samplers:\n",
    "    #print sampler.filename\n",
    "    sampler_of_filename[sampler.filename] = sampler\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_images = []\n",
    "\n",
    "for (f, l) in file_loss_tups:\n",
    "    im = sampler_of_filename[f].sample_full()\n",
    "    wrong_images.append(im)\n",
    "    \n",
    "wrong_images = util.unzero_center(wrong_images, P.MEAN_PIXEL)\n",
    "wrong_images = wrong_images.transpose(0,2,3,1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions that are most wrong\n",
    "(Number in the title of the images is the loss of that prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "from dataset import label_name\n",
    "\n",
    "\n",
    "#For N by N plots\n",
    "N = 3\n",
    "\n",
    "\n",
    "for plot_number in range(0, len(wrong_images), N**2):\n",
    "    \n",
    "    if plot_number//N**2 >= 2: #Lets not plot too many\n",
    "        break\n",
    "        \n",
    "    f, axarr = plt.subplots(N,N,figsize=(16,16))\n",
    "    \n",
    "    for i in range(min(N**2, len(wrong_images)-plot_number)):\n",
    "        \n",
    "        x = int(i%N)\n",
    "        y = int(i/N)\n",
    "        \n",
    "        i = plot_number+i\n",
    "        axarr[y,x].imshow(wrong_images[i])\n",
    "        axarr[y,x].set_title(label_name(wrong_labels[i])+\", is actually \" + label_name(actual_labels[i]) + \" \" + str(losses[i]))\n",
    "        axarr[y,x].axis('off')\n",
    "        \n",
    "    plt.subplots_adjust(wspace = -0.2, hspace=0.12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "cm = sklearn.metrics.confusion_matrix(all_targets, all_binary_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "label_names = ['Benign', 'DCIS', 'IDC']\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cm_normalized, interpolation='nearest')\n",
    "plt.xticks(np.arange(0,3), label_names)\n",
    "plt.yticks(np.arange(0,3), label_names)\n",
    "plt.tight_layout()\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From https://gist.github.com/zachguo/10296432\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels]+[7]) # 7 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print \"    \" + empty_cell,\n",
    "    for label in labels: \n",
    "        print \"%{0}s\".format(columnwidth) % label,\n",
    "    print\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print \"    %{0}s\".format(columnwidth) % label1,\n",
    "        for j in range(len(labels)): \n",
    "            cell = \"%{0}.3f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print cell,\n",
    "        print \n",
    "    \n",
    "print_cm(cm_normalized, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
